---
phase: 02-global-cb-collectors
plan: 03
type: execute
wave: 1
depends_on: []
files_modified: [src/liquidity/collectors/boe.py, tests/integration/test_global_cb_collectors.py]
autonomous: true
---

<objective>
Create Bank of England collector via weekly report scraping.

Purpose: FRED series BOEBSTAUKA was discontinued in 2016. BoE database API returns 403 for programmatic access. Alternative: scrape weekly report HTML pages to extract balance sheet data.

Discovery findings:
- Database CSV endpoint (iadb-fromshowcolumns.asp) returns 403 Access Denied
- Weekly report pages are accessible: e.g., /weekly-report/2025/26-november-2025
- Total assets ~£848 billion (Nov 2025), must be calculated by summing components
- Components: Loan to APF (£558bn), Repos (£158bn), Foreign reserves (£19bn), etc.

Output: BOECollector class that scrapes BoE weekly report HTML, registered with collector registry.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-global-cb-collectors/02-RESEARCH.md

# Prior work
@.planning/phases/01-foundation-core-data/01-02-SUMMARY.md

# Source files - patterns to follow
@src/liquidity/collectors/fred.py
@src/liquidity/collectors/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create BOECollector class with weekly report scraping</name>
  <files>src/liquidity/collectors/boe.py</files>
  <action>
Create BOECollector that scrapes weekly report HTML pages:

```python
"""Bank of England collector via weekly report scraping.

FRED series BOEBSTAUKA discontinued 2016. BoE database API returns 403.
This collector scrapes weekly report HTML pages for balance sheet data.

Weekly report URL pattern:
https://www.bankofengland.co.uk/weekly-report/YYYY/DD-month-YYYY
"""

import asyncio
import logging
import re
from datetime import datetime

import httpx
import pandas as pd
from bs4 import BeautifulSoup

from liquidity.collectors.base import BaseCollector, CollectorFetchError
from liquidity.collectors.registry import registry

logger = logging.getLogger(__name__)

# Weekly report base URL
BOE_WEEKLY_REPORT_URL = "https://www.bankofengland.co.uk/weekly-report"

# Asset components to sum for total assets (in millions GBP)
ASSET_COMPONENTS = [
    "Loan to Asset Purchase Facility",
    "Short-term repo",
    "Indexed long-term repo",
    "Term Funding Scheme",
    "Sterling bond holdings",
    "Foreign currency reserves",
]

UNIT_MAP: dict[str, str] = {
    "BOE_TOTAL_ASSETS": "millions_gbp",
}


class BOECollector(BaseCollector[pd.DataFrame]):
    """Bank of England collector via weekly report scraping."""

    def __init__(self, name: str = "boe", **kwargs):
        super().__init__(name=name, **kwargs)

    async def collect(
        self,
        start_date: datetime | None = None,
        end_date: datetime | None = None,
    ) -> pd.DataFrame:
        """Collect BoE data by scraping weekly report pages."""
        async def _fetch():
            async with httpx.AsyncClient(timeout=30.0) as client:
                # Fetch latest weekly report index page
                index_url = f"{BOE_WEEKLY_REPORT_URL}/balance-sheet-and-weekly-report"
                response = await client.get(index_url)
                response.raise_for_status()

                # Find latest report link
                soup = BeautifulSoup(response.text, 'lxml')
                report_links = soup.find_all('a', href=re.compile(r'/weekly-report/\d{4}/'))

                if not report_links:
                    raise CollectorFetchError("No weekly report links found")

                # Fetch latest report
                latest_url = f"https://www.bankofengland.co.uk{report_links[0]['href']}"
                report_response = await client.get(latest_url)
                report_response.raise_for_status()

                return self._parse_weekly_report(report_response.text)

        return await self.fetch_with_retry(_fetch)

    def _parse_weekly_report(self, html: str) -> pd.DataFrame:
        """Parse weekly report HTML and extract total assets."""
        soup = BeautifulSoup(html, 'lxml')

        # Extract date from page
        # Extract asset values from tables
        # Sum components to get total assets
        # Return standardized format
        ...


registry.register("boe", BOECollector)
```

Key implementation notes:
1. Scrape weekly report HTML (database API returns 403)
2. Total assets = sum of asset components
3. Latest total: ~£848 billion (848,000 mn GBP)
4. Weekly data published every Thursday
5. Requires beautifulsoup4 + lxml (added in 02-05)
  </action>
  <verify>uv run python -c "from liquidity.collectors.boe import BOECollector; c = BOECollector(); print(c)"</verify>
  <done>BOECollector class exists with scraping implementation</done>
</task>

<task type="auto">
  <name>Task 2: Implement weekly report HTML parsing</name>
  <files>src/liquidity/collectors/boe.py</files>
  <action>
Implement _parse_weekly_report() to extract total assets:

1. Fetch a sample weekly report page and analyze HTML structure
2. Extract numeric values from the Sterling Assets and Foreign Currency sections
3. Sum components to calculate total assets

Example URL: https://www.bankofengland.co.uk/weekly-report/2025/26-november-2025

Known components from discovery (Nov 2025, values in £mn):
- Loan to Asset Purchase Facility: 558,069
- Short-term repo: 96,634
- Indexed long-term repo: 61,479
- Term Funding Scheme: 41,894
- Sterling bond holdings: 13,230
- Foreign currency reserves: 19,122
- Total: ~848,000 (£848 billion)

Implementation should:
1. Find numeric values using regex or BeautifulSoup
2. Map values to component names
3. Sum for total assets
4. Extract report date from page
5. Return DataFrame with: timestamp, series_id, source, value, unit
  </action>
  <verify>uv run python -c "from liquidity.collectors.boe import BOECollector; import asyncio; c = BOECollector(); asyncio.run(c.collect())"</verify>
  <done>Weekly report parsing implemented and extracting correct total</done>
</task>

<task type="auto">
  <name>Task 3: Add integration tests for BOECollector</name>
  <files>tests/integration/test_global_cb_collectors.py</files>
  <action>
Add integration tests for BOECollector:

```python
class TestBOECollector:
    """Tests for Bank of England collector."""

    @pytest.mark.integration
    async def test_boe_collect_balance_sheet(self):
        """Test fetching BoE balance sheet data."""
        collector = BOECollector()
        df = await collector.collect()

        assert not df.empty
        assert "timestamp" in df.columns
        assert "value" in df.columns
        assert "unit" in df.columns
        assert df["source"].iloc[0] == "boe"

    @pytest.mark.integration
    async def test_boe_data_format(self):
        """Test BoE data has expected format."""
        collector = BOECollector()
        df = await collector.collect()

        # Values should be positive (millions GBP)
        assert (df["value"] > 0).all()
        # Unit should be millions_gbp
        assert df["unit"].iloc[0] == "millions_gbp"
```

Tests should:
1. Verify weekly report scraping works
2. Verify total assets calculated correctly (~£800-900bn range)
3. Verify units are millions_gbp
4. Handle network errors gracefully (skip if BoE website unavailable)
  </action>
  <verify>uv run pytest tests/integration/test_global_cb_collectors.py::TestBOECollector -v --tb=short</verify>
  <done>Integration tests exist and pass</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `uv run ruff check src/liquidity/collectors/boe.py` passes
- [ ] `uv run mypy src/liquidity/collectors/boe.py` passes
- [ ] `uv run pytest tests/integration/test_global_cb_collectors.py -v -k boe` passes
- [ ] BOECollector successfully fetches BoE balance sheet data
</verification>

<success_criteria>

- BOECollector class follows BaseCollector pattern
- Weekly report scraping works (HTML parsing)
- Total assets calculated by summing components (~£848bn)
- Data normalized to standard format with GBP units
- Integration tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-global-cb-collectors/02-03-SUMMARY.md`
</output>
